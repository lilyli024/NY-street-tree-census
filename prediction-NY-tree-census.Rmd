---
title: "Prediction Model using NY Street Tree Census"
author: "Lily Li"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

## About the Data

This data consists of over 680 observations and includes measurements of perceived health for each individual tree. This urban forestry data was collected by over 2,200 volunteers from NYC Parks & Recreation and partner organizations. 

Variables used for predictions:

* `health`: Indicates health of the tree; levels: poor, fair, good
* `latitude`: Latitude of tree location, in decimal degrees
* `longitude`: Longitude of tree location, in decimal degrees
* `tree_dbh`: Diameter of the tree, measured at approximately 54 inches above the ground
* `curb_loc`: Indicates trees relationship to the curb; levels: OnCurb, OffsetFromCurb
* `spc_common`: Common name of tree species
* `root_stone`: Root problems caused by paving stones in the tree bed
* `root_grate`: Root problems caused by metal grates
* `root_other`: Presence of other root problems
* `trunk_wire`: Indicates the presence of a trunk problem caused by wires or rope wrapped around the trunk
* `trnk_light`: Indicates the presence of a trunk problem caused by lighting installed on the tree
* `trnk_other`: Indicates the presence of other trunk problems
* `brch_light`: Indicates the presence of a branch problem caused by lights or wires in the branches
* `brch_shoe`: Indicates the presence of a branch problem caused by sneakers in the branches
* `brch_other`: Indicates the presence of other branch problems
* `steward`: Indicates the number of unique signs of stewardship (maintenance) observed for this tree

## Why is this important?

Due to the long time for a tree to decompose, trees can appear alive but are actually dead. Death of trees can be due to many factors, such as road construction, age, warming temperatures, and lack of maintenance. Other than serving aesthetic purposes, trees in urban areas reduce air pollution by catching particulate matter and absorbing of gaseous pollutants. On top of that, trees buffer noise, filter storm water, cool city streets during summer months by providing shade, and take in carbon dioxide from city traffic. Knowing the number of trees degrading in health and being about to make future predictions can encourage replanting efforts, stricter maintenance of trees, and financing of preservation projects.

## Goal

Using the 2015 street tree census from the NYC Parks & Recreation, my goal is to predict the health of trees given that they are alive. A tree's health can be classified as Poor, Fair, or Good.

## Loading the Data and Packages
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(ggpubr) # EDA
library(kknn) # K-nearest neighbors
library(glmnet) # linear regression
library(xgboost) # boosted trees
library(ranger) # random forests
library(vip) # variable importance
library(usethis)
usethis::edit_r_environ("project") # allocate memory to fit knn model
tidymodels_prefer()
raw_data <- read_csv('data/2015-street-tree-census-tree-data.csv')
```

Check out the [Kaggle Dataset](https://www.kaggle.com/datasets/new-york-city/ny-2015-street-tree-census-tree-data)

# Data Cleaning

## Filtering the Data

Since dead trees are missing information, such as species and steward, and stumps only mainly provide information on root health, all observations I will be using are from alive trees. There are many location metrics in the raw data, including address, postcode, zip_city, and borough, but I will use latitude and longitude for location to reduce multicollinearity. Variables like community board and block_id, are filtered out since if they were to be factorized, there would be hundreds or thousands of levels that I do not have the computation power to use; they are also are location metrics and thus would not be greatly useful as predictors. 

```{r, message=FALSE, warning=FALSE}
clean<- raw_data %>%
  mutate(health = factor(health, levels = c("Poor", "Fair", "Good"))) %>%
  dplyr::select(health, latitude, longitude, tree_dbh, curb_loc, spc_common, c(root_stone:brch_other), steward, borough) %>% na.omit()
```


## Tidying the Data

Categorical variables were turned into factors.

```{r, message=FALSE, warning=FALSE}
clean[,colnames(clean %>% select_if(is.character))] <- lapply(clean %>% select_if(is.character),as.factor)
```

Since the data set is immensely large, I choose to remove the rarer species; 85 percent of the alive trees were kept and left us with 23 out of 132 species.

```{r, message=FALSE, warning=FALSE}
tree_count <- clean$spc_common %>% table()
remove_rare <- tree_count[tree_count>5000] # remove the rarer species (a little over 15% of the trees)
common_names <- remove_rare %>% names()
species_data <- bind_cols('Common Name of Species'=common_names, 'Count'=remove_rare)

clean <- clean %>% filter(spc_common %in% common_names)
```


# Exploratory Data Analysis

To get an idea of species as a predictor of health, I have graphed the distribution of health classifications by species. Though the Norway maple is the 5th most common tree inspected, a larger proportion of Norway maples are not in good health compared to other species.
```{r, message=FALSE, warning=FALSE}
ggplot(clean %>% filter(spc_common %in% common_names), aes(y = reorder(spc_common, spc_common, function(x) length(x)), fill = health)) +
  geom_bar(position = 'stack') +
  labs(y = "Common Name for Species", title = "Distribution of Health for Alive Trees by Most Surveyed Species")
```

We can also look at numbers to see the 10 species with the lowest proportion of trees in good health. Variations of maple trees tend to be in poor or fair health.

```{r, message=FALSE, warning=FALSE}
species_df <- data.frame(matrix(nrow = 23, ncol = 4))
index = 1
for(name in common_names){
  health_by_species <- clean %>% filter(spc_common == name)
  total_good <- health_by_species %>% filter(health == "Good")
  total_fair <- health_by_species %>% filter(health == "Fair")
  total_poor <- health_by_species %>% filter(health == "Poor")
  species_df[index,] <- c(name, nrow(total_good)/nrow(health_by_species),
                         nrow(total_fair)/nrow(health_by_species),
                         nrow(total_poor)/nrow(health_by_species))
  index <- index + 1
}

species_df$X2 <- round(as.numeric(species_df$X2), digit = 3)
species_df$X3 <- round(as.numeric(species_df$X3), digit = 3)
species_df$X4 <- round(as.numeric(species_df$X4), digit = 3)

species_df <- species_df %>% 
  rename(Species = X1, Good=X2, Fair=X3, Poor=X4)

arrange(species_df, desc(Poor), desc(Fair)) %>% head(10)
```

Here is a spatial plot showing the distribution of trees by health using longitude and latitude. The borough boundary lines are created by connect lines from points of boundary, thus they reduce/distort the boundaries. That is why there are points outside the borough boundaries, but we can still see that Manhattan and Queens are more heavily surveyed and have a larger proportion of trees in fair health compared to other boroughs.

```{r, message=FALSE, warning=FALSE}
nyc <- map_data("state", "new york") %>% 
  select(long, lat, group, id = subregion)

borough_labels <- data.frame(long=c(-73.949997, -73.971321, -73.769417, -73.8648, -74.1502), 
                             lat=c(40.650002, 40.776676, 40.71, 40.875, 40.5795), 
                             borough=c("Brooklyn", "Manhattan", "Queens", "Bronx", "Staten Island"))

ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group),fill = "white", colour = "grey80", data = nyc) +
  labs(x="Longitude", y="Latitude", title="Spatial Distribution of Health for Alive Trees") +
  geom_point(aes(x = longitude, y = latitude, colour=health), size = .3, alpha = 0.5, data=clean)+
  xlim(-74.25, -73.70) + ylim(40.50, 40.91) +
  geom_text(data = borough_labels, aes(x = long, y = lat, label = borough))
```

We can also view the distribution via barchart.

```{r, message=FALSE, warning=FALSE}
borough_data <- clean %>% select(borough, health) %>% na.omit()
ggplot(borough_data , aes(y = reorder(borough, borough, function(x) length(x)), fill = health)) +
  geom_bar(position = 'stack') +
  labs(y = "Borough", title="Distribution of Health of Alive Trees by Borough")
```

```{r, message=FALSE, warning=FALSE}
clean <- clean %>% select(-borough)

save(clean, file="~/Desktop/PSTAT 131/NY-street-tree-census/data/clean_data.rda")
```

Looking at the distribution of health by binary predictors, we see that there are predictors, root_stone brch_light, and root_other, with higher proportions of Yes's. It is interesting to note that a tree with a Yes in a problem category can still be in good health.

```{r, message=FALSE, warning=FALSE}
clean %>% pivot_longer(., clean %>% select(root_stone:brch_other) %>% colnames(), names_to = "variable", values_to = "values") %>%
  ggplot(., aes(x = health, fill = values, y =""))+
  geom_bar(stat = "identity", position = "stack") + facet_wrap(~variable) +
  labs(x="Health by Predictor", y = "count", title = "Distribution of Health by Predictor")
```

```{r}
par(mfrow=c(1,3))
poor_steward <- clean %>% filter(health=="Poor") %>% group_by(steward) %>% summarise(count= n())
pie(poor_steward[,2] %>% unlist(), labels = c("1or2", "3or4", "4orMore", "None"), init.angle = 280,
    main = "Stewards for Poor Health")

fair_steward <- clean %>% filter(health=="Fair") %>% group_by(steward) %>% summarise(count= n())
pie(fair_steward[,2] %>% unlist(), labels = c("1or2", "3or4", "4orMore", "None"), init.angle = 280,
    main = "Stewards for Fair Health")

good_steward <- clean %>% filter(health=="Good") %>% group_by(steward) %>% summarise(count= n())
pie(good_steward[,2] %>% unlist(), labels = c("1or2", "3or4", "4orMore", "None"), init.angle = 280,
    main = "Stewards for Good Health")
```


# Setting up Models
## Spliting Our Data

Here, the data to split, using 80 percent of the original data to learn on. Since the data is imbalanced (most trees are in good health), stratified sampling is used to helps reduce bias in selection and best represent each factor level to evaluate differences in groups.

```{r, message=FALSE, warning=FALSE}
set.seed(9)
split <- initial_split(clean, prop = 0.80, strata = health)
train <- training(split)
test <- testing(split)
```

441901 observations in the training set

```{r, message=FALSE, warning=FALSE}
dim(train)
```

110477 observations in the testing set

```{r, message=FALSE, warning=FALSE}
dim(test)
```

We have a very large sufficient amount of data to expose potential patterns using models.

## Creating a Recipe

We will be using the same recipe in all our models, using health as our response variable, all the same selected predictors, and same model condistions.

```{r, message=FALSE, warning=FALSE}
recipe <- recipe(health ~ ., data = train) %>%
  step_dummy(all_nominal_predictors())
```

## K-Fold Cross Validation

Instead of validation set approach, we will k-fold cross validation which will split our training set into 10 subsets, leaving one for testing and evaluation of the model, and reducing overfitting.

```{r, message=FALSE, warning=FALSE}
tree_folds <- vfold_cv(train, v = 10, strata = health)
```

# Application of Models

## K Nearest Neighbors

This distance-based algorithm estimates the likelihood of class membership for each observation. Since this does not favor one group over the other based on size, this is a strong algorithm for imbalanced data.

```{r, message=FALSE, warning=FALSE}
knn_mod <-
  # specify that the model is a k-Nearest Neighhour (kNN)
  nearest_neighbor(neighbors = tune()) %>%
  # select the package that the model coming from
  set_engine("kknn") %>%
  # choose mode
  set_mode("classification")

knn_wkflow <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(recipe)

knn_grid <- grid_regular(parameters(knn_mod), levels = 8) # Optimizing the number of neighbors, using odd numbers to avoid a tie

tune_knn <- tune_grid(
  object = knn_wkflow, 
  resamples = tree_folds, 
  grid = knn_grid
)
```

Let's take a look at which K values give us the best accuracy.

```{r, message=FALSE, warning=FALSE}
show_best(tune_knn, metric = "accuracy")
```

We can see if the optimal K value for highest area under the ROC curve.

```{r, message=FALSE, warning=FALSE}
show_best(tune_knn, metric = "roc_auc")
```

We'll select the K value by accuracy and apply it to our workflow.

```{r, message=FALSE, warning=FALSE}
best_knn <- select_best(tune_knn, metric = "accuracy")
final_knn <- finalize_workflow(knn_wkflow, best_knn)
```

After tuning the number of neighbors, we can fit the data to our training set and check the accuracy of predictions with our testing set.

```{r, message=FALSE, warning=FALSE}
knn_fit <- fit(final_knn, train)

knn_accuracy <- predict(knn_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class)
knn_accuracy
```

The model is stronger at distinguishing classifications "Good" and "Poor".

```{r, message=FALSE, warning=FALSE}
results <- augment(knn_fit, new_data = test)

knn_auc <- results %>% roc_auc(truth = health, .pred_Poor:.pred_Good)	
knn_auc
```

```{r, message=FALSE, warning=FALSE}
results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()

model_metrics <- data.frame("Model"=c("KNN"), "Accuracy"=knn_accuracy, "ROC_AUC"=knn_auc)

save(tune_knn, knn_fit, file="~/Desktop/PSTAT 131/NY-street-tree-census/data/knn_model.rda")
```

## Boosted Trees

Next we'll try boosted trees where the algorithm focuses on reducing the residuals of each step for each tree so that trees are dependent on prior trees. We'll tune: the number of tree since too many causes overfitting of the training set, the learning rate so consecutive steps have less importance, and the number of splits the trees will make.

```{r, message=FALSE, warning=FALSE}
boost_tree_model <- boost_tree(min_n = tune(),
                               learn_rate = tune(),
                               tree_depth = tune(), 
                               stop_iter = tune()) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")
# tuned for early stopping tp reduce computational power; if a certain combination is performing poorly, the model stops before reaching the set number of trees

boost_wkflow <- workflow() %>% 
  add_model(boost_tree_model) %>%
  add_recipe(recipe)

boost_param <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  stop_iter())

boost_grid <- grid_max_entropy(boost_param, size = 20)


tune_boost <- tune_grid(
  object = boost_wkflow, 
  resamples = tree_folds, 
  grid = boost_grid,
)
```

Let's look at which tuned parameters have the highest accuracy

```{r, message=FALSE, warning=FALSE}
show_best(tune_boost, metric = "accuracy") 
```

Comparing optimal paremeters with the highest area under the ROC curve

```{r, message=FALSE, warning=FALSE}
show_best(tune_boost, metric = "roc_auc")
```

Here are the results:

```{r, message=FALSE, warning=FALSE}
best_boost <- select_best(tune_boost, metric = "accuracy")
final_boost <- finalize_workflow(boost_wkflow, best_boost)

boost_tree_fit <- fit(final_boost, train)

boost_tree_accuracy <- predict(boost_tree_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class) 	
boost_tree_accuracy
```

```{r, message=FALSE, warning=FALSE}
boost_tree_results <- augment(boost_tree_fit, new_data = test) 

boost_tree_auc <- boost_tree_results %>% roc_auc(truth = health, estimate= .pred_Poor:.pred_Good)
boost_tree_auc
```

```{r, message=FALSE, warning=FALSE}
boost_tree_results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()
```

Due to imbalanced data, most incorrect health classifications are classified as Good.

```{r, message=FALSE, warning=FALSE}
boost_tree_results %>%  conf_mat(truth = health, estimate = .pred_class) %>% autoplot(type="heatmap")

model_metrics[2,] <- c("Boosted Trees", boost_tree_accuracy, boost_tree_auc)

save(tune_boost, boost_tree_fit, file="~/Desktop/PSTAT 131/NY-street-tree-census/data/boosted_trees_model.rda")
```

## Random Forests

We'll use random forests that grows and combines decision trees and takes the average outputs of trees. We'll tune number of predictors to sample for each split and the number of observations for further splitting.

```{r, message=FALSE, warning=FALSE}
rf_model <- rand_forest(min_n = tune(),
                        mtry = tune()) %>%
  set_engine("ranger", importance = "permutation") %>% # look at variable importance
  set_mode("classification")

rf_wkflow <- workflow() %>% 
  add_model(rf_model) %>%
  add_recipe(recipe)

rf_param <- parameters(
  min_n(),
  mtry = mtry(range= c(2, 15))) 

rf_grid <- grid_max_entropy(rf_param, size = 10)


tune_rf <- tune_grid(
  object = rf_wkflow, 
  resamples = tree_folds, 
  grid = rf_grid,
)

```

We can glance at the optimal parameters.

```{r, message=FALSE, warning=FALSE}
show_best(tune_rf, metric = "accuracy") 
show_best(tune_rf, metric = "roc_auc")
```

```{r, message=FALSE, warning=FALSE}
best_rf <- select_best(tune_rf, metric = "accuracy")
final_rf <- finalize_workflow(rf_wkflow, best_rf)

rf_fit <- fit(final_rf, train)
```

The accuracy after tuning parameters and selecting the best model.

```{r, message=FALSE, warning=FALSE}
rf_accuracy <- predict(rf_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  yardstick::accuracy(truth=health, estimate= .pred_class)
rf_accuracy
```

```{r, message=FALSE, warning=FALSE}
rf_results <- augment(rf_fit, new_data = test) 
rf_auc <- rf_results %>% roc_auc(truth = health, estimate= .pred_Poor:.pred_Good)
rf_auc
```

```{r, message=FALSE, warning=FALSE}
rf_results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()

model_metrics[3,] <- c("Random Forests", rf_accuracy, rf_auc)

save(tune_rf, rf_fit, file="~/Desktop/PSTAT 131/NY-street-tree-census/data/rf_model.rda")
```

## LASSO 

Using multinomial linear regression, LASSO will shrink predictors that have minimal effect on the outcome down to zero. Removing insignificant variables using penalties based on lambda values will reduce random patterns from our model.

```{r, message=FALSE, warning=FALSE}
lasso_spec <- multinom_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet") %>% 
  set_mode("classification")

lasso_wkflow <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(lasso_spec)

lasso_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 10)

tune_lasso <- tune_grid(
  object = lasso_wkflow, 
  resamples = tree_folds, 
  grid = lasso_grid,
)

show_best(tune_lasso, metric = "accuracy") 
show_best(tune_lasso, metric = "roc_auc")

best_lasso <- select_best(tune_lasso, metric = "accuracy")
final_lasso <- finalize_workflow(lasso_wkflow, best_lasso)

lasso_fit <- fit(final_lasso, train)
```

Results from selecting the best penalty term for LASSO linear regression. 

```{r, message=FALSE, warning=FALSE}
lasso_accuracy <- predict(lasso_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class)
lasso_accuracy
```

```{r, message=FALSE, warning=FALSE}
lasso_results <- augment(lasso_fit, new_data = test) 
lasso_auc <- lasso_results %>% roc_auc(truth = health, estimate= .pred_Poor:.pred_Good) 
lasso_auc
```

```{r, message=FALSE, warning=FALSE}
lasso_results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()

model_metrics[4,] <- c("LASSO", lasso_accuracy, lasso_auc)

save(tune_lasso, lasso_fit, file="~/Desktop/PSTAT 131/NY-street-tree-census/data/lasso_model.rda")
```

# Conclusion

## Comparison of metrics

```{r}
model_metrics
```

Since random forests was our best model, we can look at variable importance scores. This shows us the features that are used to make accurate predictions.

```{r, message=FALSE, warning=FALSE}
rf_fit %>%
  extract_fit_parsnip() %>%
  vip()
```

