---
title: "Prediction Model using NY Street Tree Census"
author: "Lily Li"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

## About the Data

See [Kaggle Dataset](https://www.kaggle.com/datasets/new-york-city/ny-2015-street-tree-census-tree-data)

* `status`: Indicates status of the tree; levels: Alive, Dead
* `health`: Indicates health of the tree; levels: dead, poor, fair, good
* `tree_dbh`: Diameter of the tree, measured at approximately 54 inches above the ground
* `curb_loc`: Indicates trees relationship to the curb; levels: OnCurb, OffsetFromCurb
* `spc_common`: Common name of tree species
* `root_stone`: Root problems caused by paving stones in the tree bed
* `root_grate`: Root problems caused by metal grates
* `root_other`: Presence of other root problems
* `trunk_wire`: Indicates the presence of a trunk problem caused by wires or rope wrapped around the trunk
* `trnk_light`: Indicates the presence of a trunk problem caused by lighting installed on the tree
* `trnk_other`: Indicates the presence of other trunk problems
* `brch_light`: Indicates the presence of a branch problem caused by lights or wires in the branches
* `brch_shoe`: Indicates the presence of a branch problem caused by sneakers in the branches
* `brch_other`: Indicates the presence of other branch problems
* `borough`: NYC borough; levels: Bronx, Brooklyn, Manhattan, Queens, Staten Island
* `latitude`: Latitude of tree location, in decimal degrees
* `longitude`: Longitude of tree location, in decimal degrees

## Goal

## Loading Packages
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(kknn) 
library(xgboost) # for boosted trees
library(ranger)
library(usethis) 
usethis::edit_r_environ("project") # allocate memory to fit knn model
tidymodels_prefer()
raw_data <- read_csv('2015-street-tree-census-tree-data.csv')
```

# Data Cleaning
```{r, message=FALSE, warning=FALSE}
clean_with_status <- raw_data %>%
  filter(status != 'Stump') %>%
  mutate(health = ifelse(status == "Alive", as.character(health), as.character(status))) %>%
  mutate(health = factor(health, levels = c("Dead", "Poor", "Fair", "Good"))) %>%
  dplyr::select(status, health, tree_dbh, curb_loc, spc_common, c(root_stone:brch_other), borough, latitude, longitude) %>% head(10000)


# make categorical variables into factors
clean_with_status[,colnames(clean_with_status %>% select_if(is.character))] <- lapply(clean_with_status %>% select_if(is.character),as.factor)

clean <- clean_with_status %>% select(-status) %>% 
  mutate(health = factor(health, levels = c("Poor", "Fair", "Good"))) %>% na.omit() 

```

# Exploratory Data Analysis

Visualization of Tree Species Distribution
```{r}
# Ignoring NA values, there are 652,169 trees with species identified
# if tree is dead the following columns contain NA values: spc_common, steward, guards, sidewalk
tree_count <- clean$spc_common %>% table()
remove_rare <- tree_count[tree_count>5000] # remove the rarer species (a little over 15% of the trees)
common_names <- remove_rare %>% names()
species_data <- bind_cols('Common Name of Species'=common_names, 'Count'=remove_rare)

clean <- clean %>% filter(spc_common %in% common_names)
                          
ggplot(clean %>% filter(spc_common %in% common_names), aes(y = reorder(spc_common, spc_common, function(x) length(x)), fill = health)) +
  geom_bar(position = 'stack') +
  labs(y = "Common Name for Species", title = "Distribution of Health for Alive Trees by Most Surveyed Species")
```

```{r, warning=FALSE}
nyc <- map_data("state", "new york") %>% 
  select(long, lat, group, id = subregion)

borough_labels <- data.frame(long=c(-73.949997, -73.971321, -73.769417, -73.8648, -74.1502), 
                             lat=c(40.650002, 40.776676, 40.71, 40.875, 40.5795), 
                             borough=c("Brooklyn", "Manhattan", "Queens", "Bronx", "Staten Island"))

ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group),fill = "white", colour = "grey80", data = nyc) +
  labs(x="Longitude", y="Latitude", title="Spatial Distribution of Health for Alive Trees") +
  geom_point(aes(x = longitude, y = latitude, colour=health), size = .3, alpha = 0.5, data=clean)+
  xlim(-74.25, -73.70) + ylim(40.50, 40.91) +
  geom_text(data = borough_labels, aes(x = long, y = lat, label = borough))
```
```{r}
borough_data <- clean %>% select(borough, health) %>% na.omit()
ggplot(borough_data , aes(y = reorder(borough, borough, function(x) length(x)), fill = health)) +
  geom_bar(position = 'stack') +
  labs(y = "Borough", title="Distribution of Health of Alive Trees by Borough")
```

# Setting up Models
## Spliting Our Data
```{r}
set.seed(9)
split <- initial_split(clean, prop = 0.80, strata = health)
train <- training(split)
test <- testing(split)
```

```{r}
dim(train)
```

```{r}
dim(test)
```

## Creating a Recipe
```{r}
recipe <- recipe(health ~ ., data = train) %>%
  step_dummy(all_nominal_predictors())
```

## K-Fold Cross Validation
```{r}
tree_folds <- vfold_cv(train, v = 10, strata = health)
```

# Application of Models

## K Nearest Neighbors

```{r}
knn_mod <-
  # specify that the model is a k-Nearest Neighhour (kNN)
  nearest_neighbor(neighbors = tune()) %>%
  # select the package that the model coming from
  set_engine("kknn") %>%
  # choose mode
  set_mode("classification")

knn_wkflow <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(recipe)

knn_grid <- grid_regular(parameters(knn_mod), levels = 10)

tune_knn <- tune_grid(
  object = knn_wkflow, 
  resamples = tree_folds, 
  grid = knn_grid
)

show_best(tune_knn, metric = "accuracy") 
show_best(tune_knn, metric = "roc_auc")
best_knn <- select_best(tune_knn, metric = "accuracy")
final_knn <- finalize_workflow(knn_wkflow, best_knn)

knn_fit <- fit(final_knn, train)

predict(knn_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class) # 0.7377539		

results <- augment(knn_fit, new_data = test)
results %>% roc_auc(truth = health, .pred_Poor:.pred_Good) # 0.6637862		
results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()
# stronger at distinguishing classifications "Good" and "Poor"


model_metrics <- data.frame("Model"=c("KNN"), "Accuracy"=c(0.7377539), "ROC_AUC"=c(0.6637862))
```

## Boosted Trees

```{r}
boost_tree_model <- boost_tree(min_n = tune(),
                               learn_rate = tune(),
                               tree_depth = tune(),
                               stop_iter = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wkflow <- workflow() %>% 
  add_model(boost_tree_model) %>%
  add_recipe(recipe)

boost_param <- parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  stop_iter())

boost_grid <- grid_max_entropy(boost_param, size = 15)


tune_boost <- tune_grid(
  object = boost_wkflow, 
  resamples = tree_folds, 
  grid = boost_grid,
)

show_best(tune_boost, metric = "accuracy") 
show_best(tune_boost, metric = "roc_auc")

best_boost <- select_best(tune_boost, metric = "accuracy")
final_boost <- finalize_workflow(boost_wkflow, best_boost)

boost_tree_fit <- fit(final_boost, train)

boost_tree_pred <- predict(boost_tree_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class) # 0.7536232			

boost_tree_results <- augment(boost_tree_fit, new_data = test) 
boost_tree_results %>% roc_auc(truth = health, estimate= .pred_Poor:.pred_Good) # 0.6336412			
boost_tree_results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()
boost_tree_results %>%  conf_mat(truth = health, estimate = .pred_class) %>% autoplot(type="heatmap")

model_metrics[2,] <- c("Boosted Trees", 0.7536232, 0.6352137)
```

Due to imbalanced data, most incorrect health classifications are classified as Good.

## Random Forests
```{r}
rf_model <- rand_forest(min_n = tune(),
                        mtry = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_wkflow <- workflow() %>% 
  add_model(rf_model) %>%
  add_recipe(recipe)

rf_param <- parameters(
  min_n(),
  mtry = mtry(range= c(2, 14))) 

rf_grid <- grid_max_entropy(rf_param, size = 5)


tune_rf <- tune_grid(
  object = rf_wkflow, 
  resamples = tree_folds, 
  grid = rf_grid,
)

show_best(tune_rf, metric = "accuracy") 
show_best(tune_rf, metric = "roc_auc")

best_rf <- select_best(tune_rf, metric = "accuracy")
final_rf <- finalize_workflow(rf_wkflow, best_rf)

rf_fit <- fit(final_rf, train)

predict(rf_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class) # 0.7496252		

rf_results <- augment(rf_fit, new_data = test) 
rf_results %>% roc_auc(truth = health, estimate= .pred_Poor:.pred_Good) # 	
rf_results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()

model_metrics[3,] <- c("Random Forests", 0.744325, 0.6996146)
```


