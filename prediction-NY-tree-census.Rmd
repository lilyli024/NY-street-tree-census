---
title: "Prediction Model using NY Street Tree Census"
author: "Lily Li"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction

## About the Data

See [Kaggle Dataset](https://www.kaggle.com/datasets/new-york-city/ny-2015-street-tree-census-tree-data)

* `status`: Indicates status of the tree; levels: Alive, Dead
* `health`: Indicates health of the tree; levels: dead, poor, fair, good
* `tree_dbh`: Diameter of the tree, measured at approximately 54 inches above the ground
* `curb_loc`: Indicates trees relationship to the curb; levels: OnCurb, OffsetFromCurb
* `spc_common`: Common name of tree species
* `root_stone`: Root problems caused by paving stones in the tree bed
* `root_grate`: Root problems caused by metal grates
* `root_other`: Presence of other root problems
* `trunk_wire`: Indicates the presence of a trunk problem caused by wires or rope wrapped around the trunk
* `trnk_light`: Indicates the presence of a trunk problem caused by lighting installed on the tree
* `trnk_other`: Indicates the presence of other trunk problems
* `brch_light`: Indicates the presence of a branch problem caused by lights or wires in the branches
* `brch_shoe`: Indicates the presence of a branch problem caused by sneakers in the branches
* `brch_other`: Indicates the presence of other branch problems
* `borough`: NYC borough; levels: Bronx, Brooklyn, Manhattan, Queens, Staten Island
* `latitude`: Latitude of tree location, in decimal degrees
* `longitude`: Longitude of tree location, in decimal degrees

## Goal

## Loading Packages
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(kknn)
library(usethis) 
usethis::edit_r_environ("project") # allocate memory to fit knn model
tidymodels_prefer()
raw_data <- read_csv('2015-street-tree-census-tree-data.csv')
```

# Data Cleaning
```{r, message=FALSE, warning=FALSE}
clean_with_status <- raw_data %>%
  filter(status != 'Stump') %>%
  mutate(health = ifelse(status == "Alive", as.character(health), as.character(status))) %>%
  mutate(health = factor(health, levels = c("Dead", "Poor", "Fair", "Good"))) %>%
  dplyr::select(status, health, tree_dbh, curb_loc, spc_common, c(root_stone:brch_other), borough, latitude, longitude) %>% head(10000)


# make categorical variables into factors
clean_with_status[,colnames(clean_with_status %>% select_if(is.character))] <- lapply(clean_with_status %>% select_if(is.character),as.factor)

clean <- clean_with_status %>% select(-status)

clean_alive <- clean %>% na.omit() 

clean_numeric <- clean
factor_to_numeric <- function(col_data) {
  for(col in colnames(col_data)){
    col_data <- col_data %>% mutate(col = factor(col, levels = c("Yes", "No")))
  }
  return(col_data)
}

binary_cols <- clean_numeric %>% select(root_stone:brch_other)
clean_numeric[,colnames(binary_cols)] <- lapply(binary_cols, factor_to_numeric) # 2 is Yes, 1 is No
clean_numeric[,colnames(binary_cols)] <- lapply(binary_cols, as.integer)

clean_numeric <- clean_numeric %>% mutate(curb_loc = factor(curb_loc, levels = c("OffsetFromCurb","OnCurb")) %>% as.integer()) # 2 is OnCurb, and 1 is OffsetFromCurb
```

# Exploratory Data Analysis

Visualization of Tree Species Distribution
```{r}
# Ignoring NA values, there are 652,169 trees with species identified
# if tree is dead the following columns contain NA values: spc_common, steward, guards, sidewalk
tree_count <- clean_alive$spc_common %>% table()
remove_rare <- tree_count[tree_count>5000] # remove the rarer species (a little over 15% of the trees)
common_names <- remove_rare %>% names()
species_data <- bind_cols('Common Name of Species'=common_names, 'Count'=remove_rare)

ggplot(clean_alive %>% filter(spc_common %in% common_names), aes(y = reorder(spc_common, spc_common, function(x) length(x)), fill = health)) +
  geom_bar(position = 'stack') +
  labs(y = "Common Name for Species")
```

Using K Nearest Neighbors to predict the health of a tree that is alive. Predictors used are latitude and longitude
```{r}
set.seed(9)
clean <- clean_alive %>% filter(spc_common %in% common_names)
clean <- clean %>% mutate(health = factor(health, levels = c("Poor", "Fair", "Good")))
split <- initial_split(clean, prop = 0.80, strata = health)
train <- training(split)
test <- testing(split)

recipe <- recipe(health ~ ., data = train) %>%
  step_dummy(all_nominal_predictors())

knn_mod <- knn_model <-
  # specify that the model is a k-Nearest Neighhour (kNN)
  nearest_neighbor() %>%
  # select the package that the model coming from
  set_engine("kknn") %>%
  # choose mode
  set_mode("classification")

knn_wkflow <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(recipe)

knn_fit <- fit(knn_wkflow, train)

predict(knn_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class)

results <- augment(knn_fit, new_data = test)
results %>% roc_auc(truth = health, .pred_Poor:.pred_Good) # 0.6720176	
results %>% roc_curve(truth = health, .pred_Poor:.pred_Good) %>% autoplot()
# stronger at distinguishing classifications "Good"
```

```{r, warning=FALSE}
nyc <- map_data("state", "new york") %>% 
  select(long, lat, group, id = subregion)

borough_labels <- data.frame(long=c(-73.949997, -73.971321, -73.769417, -73.8648, -74.1502), 
                             lat=c(40.650002, 40.776676, 40.71, 40.875, 40.5795), 
                             borough=c("Brooklyn", "Manhattan", "Queens", "Bronx", "Staten Island"))

ggplot() +
  geom_polygon(aes(x = long, y = lat, group = group),fill = "white", colour = "grey80", data = nyc) +
  xlab('Longitude') + 
  ylab('Latitude') + 
  geom_point(aes(x = longitude, y = latitude, colour=status), size = .3, alpha = 0.5, data=clean_with_status)+
  xlim(-74.25, -73.70) + ylim(40.50, 40.91) +
  geom_text(data = borough_labels, aes(x = long, y = lat, label = borough))
```

```{r}
borough_data <- clean %>% select(borough, health) %>% na.omit()
ggplot(borough_data , aes(y = reorder(borough, borough, function(x) length(x)), fill = health)) +
  geom_bar(position = 'stack') +
  labs(y = "Borough")
```


```{r}
set.seed(9)
split <- initial_split(clean %>% select(-spc_common, -borough), prop = 0.80, strata = health)
train <- training(split)
test <- testing(split)

recipe <- recipe(health ~ ., data = train) %>%
  step_dummy(all_nominal_predictors())

knn_mod <- knn_model <-
  # specify that the model is a k-Nearest Neighhour (kNN)
  nearest_neighbor() %>%
  # select the package that the model coming from
  set_engine("kknn") %>%
  # choose mode
  set_mode("classification")

knn_wkflow <- workflow() %>%
  add_model(knn_mod) %>%
  add_recipe(recipe)

knn_fit <- fit(knn_wkflow, train)

pred <- predict(knn_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class) # 0.7965347

results <- augment(knn_fit, new_data = test) 
results %>% roc_auc(truth = health, .pred_Dead:.pred_Good) # 0.6775519
results %>% roc_curve(truth = health, .pred_Dead:.pred_Good) %>% autoplot()
# stronger at distinguishing classifications "Good"
```

Using boosted trees to predict health of a tree that is alive
```{r}
library(xgboost)
set.seed(9)
split <- initial_split(clean %>% select(-spc_common, -borough), prop = 0.80, strata = health)
train <- training(split)
test <- testing(split)

new_recipe <- recipe(health ~ . , data = train) %>%
  step_dummy(all_nominal_predictors())

boost_tree_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wkf <- workflow() %>% 
  add_model(boost_tree_model) %>%
  add_recipe(new_recipe)

boost_tree_fit <- fit(boost_wkf, train)

boost_tree_pred <- predict(boost_tree_fit, new_data = test, type = "class") %>%
  bind_cols(select(test, health)) %>%
  accuracy(truth=health, estimate= .pred_class) # 0.8139226

boost_tree_results <- augment(boost_tree_fit, new_data = test) 

boost_tree_results %>% roc_auc(truth = health, estimate= .pred_Dead:.pred_Good) # 0.6529426	
boost_tree_results %>% roc_curve(truth = health, .pred_Dead:.pred_Good) %>% autoplot()
boost_tree_results %>%  conf_mat(truth = health, estimate = .pred_class) %>% autoplot(type="heatmap") # incorrectly predicts "Good" for many trees in poor or fair health
```

```{r}
library(corrplot)
corrplot(cor(select_if(clean_numeric, is.numeric)), method="color", type="lower") # longitude and latitude seem to be the only predictors with moderately strong correlations with one another. coerrelation coeff is 0.57
```


